# -*- coding: utf-8 -*-
"""Assignment_Solution.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BNRAPj3gsgxQhoQ7pT6Hu1NYt7ivm4dj
"""

import pandas as pd
import requests
from bs4 import BeautifulSoup
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from nltk.sentiment import SentimentIntensityAnalyzer
from textblob import TextBlob
import syllables

# Set up NLTK resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('vader_lexicon')


# Read the input Excel file
input_df = pd.read_excel('/content/Task/Input.xlsx')

# Initialize lists to store the computed variables
positive_score = []
negative_score = []
polarity_score = []
subjectivity_score = []
avg_sentence_length = []
percentage_complex_words = []
fog_index = []
avg_words_per_sentence = []
complex_word_count = []
word_count = []
syllables_per_word = []
personal_pronouns = []
avg_word_length = []

# Iterate through each row in the input dataframe
for index, row in input_df.iterrows():
    url_id = row['URL_ID']
    url = row['URL']

    # Send a GET request to the URL
    response = requests.get(url)

    # Parse the HTML content using BeautifulSoup
    soup = BeautifulSoup(response.content, 'html.parser')

    # Find the article title
    title_element = soup.find('h1')
    if title_element:
        title = title_element.text.strip()
    else:
        title = ''

    # Find the article text
    article_text = ''
    article_element = soup.find('article')
    if article_element:
        paragraphs = article_element.find_all('p')
        article_text = ' '.join([p.text.strip() for p in paragraphs])

    # Save the extracted article in a text file
    filename = f'/content/Task/Extract text/{url_id}.txt'
    with open(filename, 'w', encoding='utf-8') as file:
        file.write(f'{title}\n\n{article_text}')

    # Perform textual analysis and compute the variables

    # Sentiment analysis using VADER
    analyzer = SentimentIntensityAnalyzer()
    sentiment_scores = analyzer.polarity_scores(article_text)
    positive_score.append(sentiment_scores['pos'])
    negative_score.append(sentiment_scores['neg'])
    polarity_score.append(sentiment_scores['compound'])

    # Sentiment analysis using TextBlob
    blob = TextBlob(article_text)
    subjectivity_score.append(blob.sentiment.subjectivity)

    # Tokenize sentences
    sentences = sent_tokenize(article_text)
    num_sentences = len(sentences)

    if num_sentences > 0:
      # Average sentence length
      total_sentence_length = sum(len(sentence.split()) for sentence in sentences)
      avg_sentence_length.append(total_sentence_length / num_sentences)
    else:
      avg_sentence_length.append(0)

    # Percentage of complex words
    words = word_tokenize(article_text)
    stopwords_set = set(stopwords.words('english'))
    complex_words = [word for word in words if word.lower() not in stopwords_set and len(word) > 2]
    if len(words) > 0:
        percentage_complex_words.append(len(complex_words) / len(words) * 100)
    else:
        percentage_complex_words.append(0)

    # FOG index
    num_complex_words = len(complex_words)
    if len(words) > 0:
        fog_index.append(0.4 * ((avg_sentence_length[-1] + (num_complex_words / len(words) * 100))))
    else:
        fog_index.append(0)


    if num_sentences > 0:
      # Average number of words per sentence
      avg_words_per_sentence.append(len(words) / num_sentences)
    else:
      avg_words_per_sentence.append(0)

    # Complex word count
    complex_word_count.append(num_complex_words)

    # Word count
    word_count.append(len(words))

    # Syllables per word
    syllable_count = sum(syllables.estimate(word) for word in words)
    if len(words) > 0:
        syllables_per_word.append(syllable_count / len(words))
    else:
        syllables_per_word.append(0)

    # Personal pronouns
    personal_pronouns.append(len([word for word in words if word.lower() in ['i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours']]))

    # Average word length
    if len(words) > 0:
        avg_word_length.append(sum(len(word) for word in words) / len(words))
    else:
        avg_word_length.append(0)

    print(f'Computed variables for {filename}')

# Read the output structure Excel file
output_df = pd.read_excel('/content/Task/Output Data Structure.xlsx')

# Update the output structure dataframe with the computed variables
output_df['POSITIVE SCORE'] = positive_score
output_df['NEGATIVE SCORE'] = negative_score
output_df['POLARITY SCORE'] = polarity_score
output_df['SUBJECTIVITY SCORE'] = subjectivity_score
output_df['AVG SENTENCE LENGTH'] = avg_sentence_length
output_df['PERCENTAGE OF COMPLEX WORDS'] = percentage_complex_words
output_df['FOG INDEX'] = fog_index
output_df['AVG NUMBER OF WORDS PER SENTENCE'] = avg_words_per_sentence
output_df['COMPLEX WORD COUNT'] = complex_word_count
output_df['WORD COUNT'] = word_count
output_df['SYLLABLE PER WORD'] = syllables_per_word
output_df['PERSONAL PRONOUNS'] = personal_pronouns
output_df['AVG WORD LENGTH'] = avg_word_length

# Save the updated output dataframe to a new Excel file
output_df.to_excel('/content/Task/Computed Variables.xlsx', index=False)

print('Text extraction and analysis completed.')

